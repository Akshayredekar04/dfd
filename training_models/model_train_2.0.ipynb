{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36076/2580772868.py:258: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend('soundfile')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "class AudioDeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dirs, sample_rate=16000, max_length=4.0):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = int(max_length * sample_rate)\n",
    "        \n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for data_dir in data_dirs:\n",
    "            data_dir = Path(data_dir)\n",
    "            label = 0 if 'real' in data_dir.name.lower() else 1\n",
    "            for audio_file in data_dir.glob('*.wav'):\n",
    "                self.audio_files.append(str(audio_file))\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        assert len(self.audio_files) > 0, \"No audio files found in the provided directories.\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        if orig_sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "        \n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples > self.max_samples:\n",
    "            waveform = waveform[:, :self.max_samples]\n",
    "        elif num_samples < self.max_samples:\n",
    "            padding = torch.zeros(1, self.max_samples - num_samples)\n",
    "            waveform = torch.cat([waveform, padding], dim=1)\n",
    "        \n",
    "        return waveform.squeeze(0), label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = torch.stack([wf for wf in waveforms])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return waveforms, labels\n",
    "\n",
    "def get_dataloaders(train_dirs, val_dirs, test_dirs, batch_size=16, num_workers=8):\n",
    "    train_dataset = AudioDeepfakeDataset(train_dirs)\n",
    "    val_dataset = AudioDeepfakeDataset(val_dirs)\n",
    "    test_dataset = AudioDeepfakeDataset(test_dirs)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "class AudioDeepfakeModel(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=2):\n",
    "        super(AudioDeepfakeModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.wav2vec2.wav2vec2.feature_extractor.eval()\n",
    "        for param in self.wav2vec2.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.wav2vec2(input_values, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "def train_model(model, train_loader, val_loader, output_dir, num_epochs=15, patience=5, accum_steps=2):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=500)\n",
    "    scaler = GradScaler()  # For mixed precision training\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    best_f1 = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear gradients at the start of each epoch\n",
    "        for i, batch in enumerate(train_pbar):\n",
    "            waveforms, labels = batch\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "            \n",
    "            with autocast():  # Mixed precision context\n",
    "                outputs = model(waveforms, labels=labels)\n",
    "                loss = outputs.loss / accum_steps  # Normalize loss for accumulation\n",
    "            \n",
    "            scaler.scale(loss).backward()  # Scale gradients for mixed precision\n",
    "            \n",
    "            if (i + 1) % accum_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item() * accum_steps  # Unscale loss for logging\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                \"loss\": f\"{train_loss/train_total:.4f}\",\n",
    "                \"acc\": f\"{train_correct/train_total:.4f}\"\n",
    "            })\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                waveforms, labels = batch\n",
    "                waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(waveforms, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.logits.argmax(dim=-1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    \"loss\": f\"{val_loss/val_total:.4f}\",\n",
    "                    \"acc\": f\"{val_correct/val_total:.4f}\"\n",
    "                })\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Compute F1-score\n",
    "        metrics = compute_metrics(val_labels, val_preds)\n",
    "        val_f1 = metrics[\"f1\"]\n",
    "        \n",
    "        # Save best model based on F1\n",
    "        if epoch == 0 or val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            model.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "            processor.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "def plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Val Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Set audio backend\n",
    "try:\n",
    "    torchaudio.set_audio_backend('soundfile')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Define directories\n",
    "train_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\"\n",
    "]\n",
    "val_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\"\n",
    "]\n",
    "test_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "]\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_dirs,\n",
    "    val_dirs,\n",
    "    test_dirs,\n",
    "    batch_size=32,  # Increased from 8\n",
    "    num_workers=8   # Increased from 4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_36076/2580772868.py:127: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision training\n",
      "Epoch 1/15 [Train]:   0%|          | 0/1001 [00:00<?, ?it/s]/tmp/ipykernel_36076/2580772868.py:145: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision context\n",
      "Epoch 1/15 [Train]: 100%|██████████| 1001/1001 [10:51<00:00,  1.54it/s, loss=0.0108, acc=0.8392]\n",
      "Epoch 1/15 [Val]:   0%|          | 0/126 [00:00<?, ?it/s]/tmp/ipykernel_36076/2580772868.py:183: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/15 [Val]: 100%|██████████| 126/126 [00:21<00:00,  5.73it/s, loss=0.0091, acc=0.8946]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AudioDeepfakeModel' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping and optimizations\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot metrics\u001b[39;00m\n\u001b[1;32m     14\u001b[0m plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir)\n",
      "Cell \u001b[0;32mIn[10], line 211\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, output_dir, num_epochs, patience, accum_steps)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m val_f1 \u001b[38;5;241m>\u001b[39m best_f1:\n\u001b[1;32m    210\u001b[0m     best_f1 \u001b[38;5;241m=\u001b[39m val_f1\n\u001b[0;32m--> 211\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    212\u001b[0m     processor\u001b[38;5;241m.\u001b[39msave_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Early stopping based on validation loss\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AudioDeepfakeModel' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AudioDeepfakeModel(model_name=\"facebook/wav2vec2-base\", num_labels=2)\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"./deepfake_model_checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train the model with early stopping and optimizations\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, output_dir, num_epochs=15, patience=5, accum_steps=2\n",
    ")\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
