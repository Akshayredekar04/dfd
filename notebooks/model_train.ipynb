{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import soundfile as sf\n",
    "from multiprocessing import Pool\n",
    "import uuid\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def validate_wav(file_path):\n",
    "    \"\"\"Validate if a WAV file is readable and non-empty.\"\"\"\n",
    "    try:\n",
    "        data, sr = sf.read(file_path)\n",
    "        if len(data) == 0:\n",
    "            raise ValueError(\"Empty audio file\")\n",
    "        return file_path, True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Invalid file {file_path}: {str(e)}\")\n",
    "        return file_path, False\n",
    "\n",
    "def remove_corrupted_files(directory):\n",
    "    \"\"\"Remove corrupted WAV files from the directory and return count of valid files.\"\"\"\n",
    "    try:\n",
    "        # Get list of WAV files\n",
    "        files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.wav')]\n",
    "        if not files:\n",
    "            logging.info(f\"No WAV files found in {directory}\")\n",
    "            return 0\n",
    "\n",
    "        # Validate files in parallel\n",
    "        with Pool() as pool:\n",
    "            results = pool.map(validate_wav, files)\n",
    "\n",
    "        # Separate valid and corrupted files\n",
    "        valid_files = [f for f, valid in results if valid]\n",
    "        corrupted_files = [f for f, valid in results if not valid]\n",
    "\n",
    "        # Remove corrupted files\n",
    "        for corrupted_file in corrupted_files:\n",
    "            try:\n",
    "                os.remove(corrupted_file)\n",
    "                logging.info(f\"Removed corrupted file: {corrupted_file}\")\n",
    "            except OSError as e:\n",
    "                logging.error(f\"Error removing {corrupted_file}: {str(e)}\")\n",
    "\n",
    "        return len(valid_files)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing directory {directory}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def rename_files_with_timestamp(directory):\n",
    "    \"\"\"Rename all WAV files in the directory with unique timestamp-based names.\"\"\"\n",
    "    try:\n",
    "        # Get list of WAV files\n",
    "        files = [f for f in os.listdir(directory) if f.endswith('.wav')]\n",
    "        if not files:\n",
    "            logging.info(f\"No WAV files to rename in {directory}\")\n",
    "            return\n",
    "\n",
    "        # Rename files\n",
    "        for idx, filename in enumerate(files):\n",
    "            # Generate unique timestamp-based name with UUID for additional uniqueness\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "            unique_id = str(uuid.uuid4())[:8]  # Short UUID segment\n",
    "            new_filename = f\"audio_{timestamp}_{unique_id}.wav\"\n",
    "\n",
    "            # Define old and new file paths\n",
    "            old_file = os.path.join(directory, filename)\n",
    "            new_file = os.path.join(directory, new_filename)\n",
    "\n",
    "            # Rename file\n",
    "            try:\n",
    "                os.rename(old_file, new_file)\n",
    "                logging.info(f\"Renamed {filename} to {new_filename}\")\n",
    "                # Small delay to ensure unique timestamps\n",
    "                time.sleep(0.001)\n",
    "            except OSError as e:\n",
    "                logging.error(f\"Error renaming {filename}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing directory {directory}: {str(e)}\")\n",
    "\n",
    "def process_directories(directories):\n",
    "    \"\"\"Process each directory: remove corrupted files, rename valid files, and count valid files.\"\"\"\n",
    "    valid_file_counts = {}\n",
    "    \n",
    "    for directory in directories:\n",
    "        logging.info(f\"Processing directory: {directory}\")\n",
    "        \n",
    "        # Step 1: Remove corrupted files and get count of valid files\n",
    "        valid_count = remove_corrupted_files(directory)\n",
    "        valid_file_counts[directory] = valid_count\n",
    "        \n",
    "        # Step 2: Rename valid files\n",
    "        rename_files_with_timestamp(directory)\n",
    "        \n",
    "    # Print valid file counts\n",
    "    logging.info(\"Valid file counts per directory:\")\n",
    "    for directory, count in valid_file_counts.items():\n",
    "        logging.info(f\"{directory}: {count} valid files\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     directories = [\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\",\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\",\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "#         \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\",\n",
    "#     ]\n",
    "    \n",
    "#     process_directories(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveforms shape: torch.Size([16, 64000])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "class AudioDeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dirs, sample_rate=16000, max_length=4.0):\n",
    "        \"\"\"\n",
    "        Custom Dataset for audio deepfake detection.\n",
    "        \n",
    "        Args:\n",
    "            data_dirs (list): List of directories containing audio files (real and fake).\n",
    "            sample_rate (int): Target sample rate (16000 Hz for wav2vec2).\n",
    "            max_length (float): Maximum audio length in seconds (4.0 seconds).\n",
    "        \"\"\"\n",
    "        self.data_dirs = data_dirs\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = int(max_length * sample_rate)\n",
    "        \n",
    "        # Collect all audio files and their labels\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for data_dir in data_dirs:\n",
    "            data_dir = Path(data_dir)\n",
    "            # Label: 0 for real, 1 for fake\n",
    "            label = 0 if 'real' in data_dir.name.lower() else 1\n",
    "            for audio_file in data_dir.glob('*.wav'):\n",
    "                self.audio_files.append(str(audio_file))\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        assert len(self.audio_files) > 0, \"No audio files found in the provided directories.\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load waveform using torchaudio\n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to 16kHz if needed\n",
    "        if orig_sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Normalize waveform (zero mean, unit variance)\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples > self.max_samples:\n",
    "            waveform = waveform[:, :self.max_samples]\n",
    "        elif num_samples < self.max_samples:\n",
    "            padding = torch.zeros(1, self.max_samples - num_samples)\n",
    "            waveform = torch.cat([waveform, padding], dim=1)\n",
    "        \n",
    "        return waveform.squeeze(0), label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length audio in batches.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (waveform, label) tuples.\n",
    "    \n",
    "    Returns:\n",
    "        waveforms: Padded waveforms as a tensor.\n",
    "        labels: Tensor of labels.\n",
    "    \"\"\"\n",
    "    waveforms, labels = zip(*batch)\n",
    "    \n",
    "    # Stack waveforms and labels\n",
    "    waveforms = torch.stack([wf for wf in waveforms])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return waveforms, labels\n",
    "\n",
    "def get_dataloaders(train_dirs, val_dirs, test_dirs, batch_size=16, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create dataloaders for train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_dirs (list): Directories for training data.\n",
    "        val_dirs (list): Directories for validation data.\n",
    "        test_dirs (list): Directories for test data.\n",
    "        batch_size (int): Batch size for dataloaders.\n",
    "        num_workers (int): Number of workers for data loading.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader: PyTorch DataLoaders.\n",
    "    \"\"\"\n",
    "    # Initialize datasets\n",
    "    train_dataset = AudioDeepfakeDataset(train_dirs)\n",
    "    val_dataset = AudioDeepfakeDataset(val_dirs)\n",
    "    test_dataset = AudioDeepfakeDataset(test_dirs)\n",
    "    \n",
    "    # Initialize dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\"\n",
    "    ]\n",
    "    val_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\"\n",
    "    ]\n",
    "    test_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "    ]\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(train_dirs, val_dirs, test_dirs, batch_size=16)\n",
    "    \n",
    "    # Test a batch\n",
    "    for waveforms, labels in train_loader:\n",
    "        print(f\"Waveforms shape: {waveforms.shape}\")  # Expected: [batch_size, 64000]\n",
    "        print(f\"Labels shape: {labels.shape}\")        # Expected: [batch_size]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def display_random_audio(directory):\n",
    "    \"\"\"Display one random WAV file from the given directory.\"\"\"\n",
    "    try:\n",
    "        # Get list of WAV files\n",
    "        files = [f for f in os.listdir(directory) if f.endswith('.wav')]\n",
    "        if not files:\n",
    "            print(f\"No WAV files found in {directory}\")\n",
    "            return\n",
    "        \n",
    "        # Select random file\n",
    "        random_file = random.choice(files)\n",
    "        file_path = os.path.join(directory, random_file)\n",
    "        \n",
    "        # Display audio\n",
    "        print(f\"Playing: {file_path}\")\n",
    "        display(Audio(file_path))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error playing audio from {directory}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directories = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "    ]\n",
    "    \n",
    "    # Display one random audio file from each directory\n",
    "    for directory in directories:\n",
    "        display_random_audio(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Section 1: Define Model Class\n",
    "class AudioDeepfakeModel(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=2):\n",
    "        super(AudioDeepfakeModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.wav2vec2.wav2vec2.feature_extractor.eval()\n",
    "        for param in self.wav2vec2.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.wav2vec2(input_values, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "# Section 2: Define Metrics Function\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Section 3: Setup DataLoaders\n",
    "train_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\"\n",
    "]\n",
    "val_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\"\n",
    "]\n",
    "test_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "]\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_dirs,\n",
    "    val_dirs,\n",
    "    test_dirs,\n",
    "    batch_size=8,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Section 4: Training Function with Progress Bar and Metrics Tracking\n",
    "def train_model(model, train_loader, val_loader, output_dir, num_epochs=3):\n",
    "    try:\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=500)\n",
    "        \n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        best_f1 = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "            train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            \n",
    "            for batch in train_pbar:\n",
    "                try:\n",
    "                    waveforms, labels = batch\n",
    "                    waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(waveforms, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    preds = outputs.logits.argmax(dim=-1)\n",
    "                    train_correct += (preds == labels).sum().item()\n",
    "                    train_total += labels.size(0)\n",
    "                    \n",
    "                    train_pbar.set_postfix({\n",
    "                        \"loss\": f\"{train_loss/train_total:.4f}\",\n",
    "                        \"acc\": f\"{train_correct/train_total:.4f}\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training batch: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_accuracy = train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0, 0, 0\n",
    "            val_preds, val_labels = [], []\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_pbar:\n",
    "                    try:\n",
    "                        waveforms, labels = batch\n",
    "                        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(waveforms, labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                        val_loss += loss.item()\n",
    "                        preds = outputs.logits.argmax(dim=-1)\n",
    "                        val_correct += (preds == labels).sum().item()\n",
    "                        val_total += labels.size(0)\n",
    "                        val_preds.extend(preds.cpu().numpy())\n",
    "                        val_labels.extend(labels.cpu().numpy())\n",
    "                        \n",
    "                        val_pbar.set_postfix({\n",
    "                            \"loss\": f\"{val_loss/val_total:.4f}\",\n",
    "                            \"acc\": f\"{val_correct/val_total:.4f}\"\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in validation batch: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            val_accuracy = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Compute F1-score\n",
    "            metrics = compute_metrics(val_labels, val_preds)\n",
    "            val_f1 = metrics[\"f1\"]\n",
    "            \n",
    "            # Save best model\n",
    "            if epoch == 0 or val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                model.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "                processor.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Section 5: Plotting Function\n",
    "def plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Val Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Section 6: Inference Function\n",
    "def predict_audio_clip(model_path, audio_path, sample_rate=16000):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = AudioDeepfakeModel.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    waveform, orig_sample_rate = torchaudio.load(audio_path)\n",
    "    if orig_sample_rate != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sample_rate, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "    \n",
    "    max_samples = int(4.0 * sample_rate)\n",
    "    if waveform.shape[1] > max_samples:\n",
    "        waveform = waveform[:, :max_samples]\n",
    "    elif waveform.shape[1] < max_samples:\n",
    "        padding = torch.zeros(1, max_samples - waveform.shape[1])\n",
    "        waveform = torch.cat([waveform, padding], dim=1)\n",
    "    \n",
    "    inputs = waveform.squeeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.unsqueeze(0))\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": \"Real\" if prediction == 0 else \"Fake\",\n",
    "        \"prob_real\": probs[0, 0].item(),\n",
    "        \"prob_fake\": probs[0, 1].item()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3 [Train]:   0%|          | 10/4002 [03:01<20:08:15, 18.16s/it, loss=0.0869, acc=0.4875]\n"
     ]
    }
   ],
   "source": [
    "# Section 7: Initialize Model and Train\n",
    "output_dir = \"./deepfake_model_checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model = AudioDeepfakeModel(model_name=\"facebook/wav2vec2-base\", num_labels=2)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, output_dir, num_epochs=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Plot Metrics\n",
    "if train_losses:\n",
    "    plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir)\n",
    "\n",
    "# Section 9: Test Inference on Example Audio Clip\n",
    "example_audio_path = \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real/example.wav\"\n",
    "result = predict_audio_clip(os.path.join(output_dir, \"best_model\"), example_audio_path)\n",
    "print(\"Inference Result:\", result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
