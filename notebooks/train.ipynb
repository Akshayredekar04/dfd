{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall torchaudio\n",
    "# pip install torchaudio --index-url https://download.pytorch.org/whl/cu128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waveforms shape: torch.Size([16, 64000])\n",
      "Labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "class AudioDeepfakeDataset(Dataset):\n",
    "    def __init__(self, data_dirs, sample_rate=16000, max_length=4.0):\n",
    "        \"\"\"\n",
    "        Custom Dataset for audio deepfake detection.\n",
    "        \n",
    "        Args:\n",
    "            data_dirs (list): List of directories containing audio files (real and fake).\n",
    "            sample_rate (int): Target sample rate (16000 Hz for wav2vec2).\n",
    "            max_length (float): Maximum audio length in seconds (4.0 seconds).\n",
    "        \"\"\"\n",
    "        self.data_dirs = data_dirs\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = int(max_length * sample_rate)\n",
    "        \n",
    "        # Collect all audio files and their labels\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for data_dir in data_dirs:\n",
    "            data_dir = Path(data_dir)\n",
    "            # Label: 0 for real, 1 for fake\n",
    "            label = 0 if 'real' in data_dir.name.lower() else 1\n",
    "            for audio_file in data_dir.glob('*.wav'):\n",
    "                self.audio_files.append(str(audio_file))\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        assert len(self.audio_files) > 0, \"No audio files found in the provided directories.\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        audio_path = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load waveform using torchaudio\n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to 16kHz if needed\n",
    "        if orig_sample_rate != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sample_rate, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Normalize waveform (zero mean, unit variance)\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        num_samples = waveform.shape[1]\n",
    "        if num_samples > self.max_samples:\n",
    "            waveform = waveform[:, :self.max_samples]\n",
    "        elif num_samples < self.max_samples:\n",
    "            padding = torch.zeros(1, self.max_samples - num_samples)\n",
    "            waveform = torch.cat([waveform, padding], dim=1)\n",
    "        \n",
    "        return waveform.squeeze(0), label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length audio in batches.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (waveform, label) tuples.\n",
    "    \n",
    "    Returns:\n",
    "        waveforms: Padded waveforms as a tensor.\n",
    "        labels: Tensor of labels.\n",
    "    \"\"\"\n",
    "    waveforms, labels = zip(*batch)\n",
    "    \n",
    "    # Stack waveforms and labels\n",
    "    waveforms = torch.stack([wf for wf in waveforms])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return waveforms, labels\n",
    "\n",
    "def get_dataloaders(train_dirs, val_dirs, test_dirs, batch_size=16, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create dataloaders for train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_dirs (list): Directories for training data.\n",
    "        val_dirs (list): Directories for validation data.\n",
    "        test_dirs (list): Directories for test data.\n",
    "        batch_size (int): Batch size for dataloaders.\n",
    "        num_workers (int): Number of workers for data loading.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader: PyTorch DataLoaders.\n",
    "    \"\"\"\n",
    "    # Initialize datasets\n",
    "    train_dataset = AudioDeepfakeDataset(train_dirs)\n",
    "    val_dataset = AudioDeepfakeDataset(val_dirs)\n",
    "    test_dataset = AudioDeepfakeDataset(test_dirs)\n",
    "    \n",
    "    # Initialize dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\"\n",
    "    ]\n",
    "    val_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\"\n",
    "    ]\n",
    "    test_dirs = [\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "        \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "    ]\n",
    "    \n",
    "    # Get dataloaders\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(train_dirs, val_dirs, test_dirs, batch_size=16)\n",
    "    \n",
    "    # Test a batch\n",
    "    for waveforms, labels in train_loader:\n",
    "        print(f\"Waveforms shape: {waveforms.shape}\")  # Expected: [batch_size, 64000]\n",
    "        print(f\"Labels shape: {labels.shape}\")        # Expected: [batch_size]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Section 1: Define Model Class\n",
    "class AudioDeepfakeModel(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/wav2vec2-base\", num_labels=2):\n",
    "        super(AudioDeepfakeModel, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.wav2vec2.wav2vec2.feature_extractor.eval()\n",
    "        for param in self.wav2vec2.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.wav2vec2(input_values, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "# Section 2: Define Metrics Function\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Section 3: Setup DataLoaders\n",
    "train_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/fake\"\n",
    "]\n",
    "val_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/val/fake\"\n",
    "]\n",
    "test_dirs = [\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real\",\n",
    "    \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/fake\"\n",
    "]\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    train_dirs,\n",
    "    val_dirs,\n",
    "    test_dirs,\n",
    "    batch_size=8,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Section 4: Training Function with Progress Bar and Metrics Tracking\n",
    "def train_model(model, train_loader, val_loader, output_dir, num_epochs=3):\n",
    "    try:\n",
    "        processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=500)\n",
    "        \n",
    "        train_losses, val_losses = [], []\n",
    "        train_accuracies, val_accuracies = [], []\n",
    "        best_f1 = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "            train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            \n",
    "            for batch in train_pbar:\n",
    "                try:\n",
    "                    waveforms, labels = batch\n",
    "                    waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(waveforms, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    preds = outputs.logits.argmax(dim=-1)\n",
    "                    train_correct += (preds == labels).sum().item()\n",
    "                    train_total += labels.size(0)\n",
    "                    \n",
    "                    train_pbar.set_postfix({\n",
    "                        \"loss\": f\"{train_loss/train_total:.4f}\",\n",
    "                        \"acc\": f\"{train_correct/train_total:.4f}\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in training batch: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_accuracy = train_correct / train_total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0, 0, 0\n",
    "            val_preds, val_labels = [], []\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_pbar:\n",
    "                    try:\n",
    "                        waveforms, labels = batch\n",
    "                        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(waveforms, labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                        val_loss += loss.item()\n",
    "                        preds = outputs.logits.argmax(dim=-1)\n",
    "                        val_correct += (preds == labels).sum().item()\n",
    "                        val_total += labels.size(0)\n",
    "                        val_preds.extend(preds.cpu().numpy())\n",
    "                        val_labels.extend(labels.cpu().numpy())\n",
    "                        \n",
    "                        val_pbar.set_postfix({\n",
    "                            \"loss\": f\"{val_loss/val_total:.4f}\",\n",
    "                            \"acc\": f\"{val_correct/val_total:.4f}\"\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in validation batch: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            val_accuracy = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            # Compute F1-score\n",
    "            metrics = compute_metrics(val_labels, val_preds)\n",
    "            val_f1 = metrics[\"f1\"]\n",
    "            \n",
    "            # Save best model\n",
    "            if epoch == 0 or val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                model.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "                processor.save_pretrained(os.path.join(output_dir, \"best_model\"))\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Section 5: Plotting Function\n",
    "def plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Val Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metrics_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Section 6: Inference Function\n",
    "def predict_audio_clip(model_path, audio_path, sample_rate=16000):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = AudioDeepfakeModel.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    waveform, orig_sample_rate = torchaudio.load(audio_path)\n",
    "    if orig_sample_rate != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sample_rate, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-6)\n",
    "    \n",
    "    max_samples = int(4.0 * sample_rate)\n",
    "    if waveform.shape[1] > max_samples:\n",
    "        waveform = waveform[:, :max_samples]\n",
    "    elif waveform.shape[1] < max_samples:\n",
    "        padding = torch.zeros(1, max_samples - waveform.shape[1])\n",
    "        waveform = torch.cat([waveform, padding], dim=1)\n",
    "    \n",
    "    inputs = waveform.squeeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.unsqueeze(0))\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": \"Real\" if prediction == 0 else \"Fake\",\n",
    "        \"prob_real\": probs[0, 0].item(),\n",
    "        \"prob_fake\": probs[0, 1].item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b6170def574711b7ef9974615c5bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df30112183942e5b3c5c1c6fb19bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ed6b3785cf4390b88dac7ea1c43a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff923a51044079bd807bb5aea99b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8b9f794cf645f29d13b0f9f0973d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e526747d0e48e0976b786f8ee1f37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f150772da09425ebcb5e26c6cc2d06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 [Train]:  11%|â–ˆ         | 429/4002 [03:59<33:12,  1.79it/s, loss=0.0697, acc=0.6888]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AudioDeepfakeModel(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, output_dir, num_epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(waveforms, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 85\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     87\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Section 7: Initialize Model and Train\n",
    "output_dir = \"./deepfake_model_checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model = AudioDeepfakeModel(model_name=\"facebook/wav2vec2-base\", num_labels=2)\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, output_dir, num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Plot Metrics\n",
    "if train_losses:\n",
    "    plot_metrics(train_losses, train_accuracies, val_losses, val_accuracies, output_dir)\n",
    "\n",
    "# Section 9: Test Inference on Example Audio Clip\n",
    "example_audio_path = \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/test/real/example.wav\"\n",
    "result = predict_audio_clip(os.path.join(output_dir, \"best_model\"), example_audio_path)\n",
    "print(\"Inference Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2802/1573931479.py:4: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend('soundfile')\n",
      "/tmp/ipykernel_2802/1573931479.py:7: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  print(f\"Current audio backend: {torchaudio.get_audio_backend()}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current audio backend: None\n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening '/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real/16555.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example: Load an audio file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real/16555.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded audio with sample rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/_backend/soundfile.py:27\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     19\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchaudio/_backend/soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    141\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msoundfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[1;32m    223\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bitrate_mode \u001b[38;5;241m=\u001b[39m bitrate_mode\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real/16555.wav': System error."
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Set the audio backend to 'soundfile'\n",
    "torchaudio.set_audio_backend('soundfile')\n",
    "\n",
    "# Verify the backend (optional)\n",
    "print(f\"Current audio backend: {torchaudio.get_audio_backend()}\")\n",
    "\n",
    "# Example: Load an audio file\n",
    "audio_path = \"/teamspace/studios/this_studio/audio_detect/dataset/split_data/train/real/16555.wav\"\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "print(f\"Loaded audio with sample rate: {sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
